"""
Selective State Space Model (Mamba) implementation.

This module implements the core selective state space model as described in
"Mamba: Linear-Time Sequence Modeling with Selective State Spaces" (Gu & Dao, 2023).
Optimized for H100 GPUs with parallel scan implementation.
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat


def parallel_scan_ref(f, x, init):
    """
    Parallel scan implementation (Blelloch 1990).
    
    This is a reference implementation that will be used if CUDA is not available.
    
    Args:
        f: Binary associative function
        x: Input tensor of shape (batch_size, seq_len, ...)
        init: Initial state
        
    Returns:
        Output tensor of shape (batch_size, seq_len, ...)
    """
    batch_size, seq_len, *rest = x.shape
    
    # Handle trivial case
    if seq_len == 1:
        return f(init, x)
    
    # Up-sweep (reduce) phase
    h = x.clone()
    for d in range(math.ceil(math.log2(seq_len))):
        mask = (torch.arange(seq_len, device=x.device) % (2 * 2**d)) == (2**d - 1)
        mask = mask.view(1, seq_len, *([1] * len(rest)))
        
        h_shifted = torch.zeros_like(h)
        h_shifted[:, 2**d:] = h[:, :-2**d]
        h = torch.where(mask, f(h, h_shifted), h)
    
    # Down-sweep phase
    g = torch.zeros_like(x)
    g[:, -1] = h[:, -1]
    for d in range(math.ceil(math.log2(seq_len)) - 1, -1, -1):
        mask = (torch.arange(seq_len, device=x.device) % (2 * 2**d)) == (2**d - 1)
        mask = mask.view(1, seq_len, *([1] * len(rest)))
        
        g_shifted = torch.zeros_like(g)
        g_shifted[:, 2**d:] = g[:, :-2**d]
        
        g = torch.where(mask, g_shifted, g)
        
        mask = (torch.arange(seq_len, device=x.device) % (2 * 2**d)) == (2 * 2**d - 1)
        mask = mask.view(1, seq_len, *([1] * len(rest)))
        
        g = torch.where(mask, f(g, h), g)
    
    # Combine with initial state
    g = f(init.unsqueeze(1), g)
    
    return g


class SelectiveSSM(nn.Module):
    """
    Selective State Space Model (Mamba) implementation.
    
    This is the core component of the BrainMamba architecture, implementing
    the selective scan operation with input-dependent parameters.
    Optimized for H100 GPUs with parallel scan implementation.
    """
    
    def __init__(
        self,
        d_model,
        d_state=64,
        dropout=0.0,
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        use_parallel_scan=True,
    ):
        """
        Initialize the Selective SSM.
        
        Args:
            d_model: Model dimension
            d_state: State dimension
            dropout: Dropout rate
            dt_min: Minimum value for the step size
            dt_max: Maximum value for the step size
            dt_init: Initialization method for the step size ("random" or "constant")
            dt_scale: Scaling factor for the step size
            dt_init_floor: Minimum value for random initialization
            use_parallel_scan: Whether to use parallel scan for faster computation
        """
        super().__init__()
        
        self.d_model = d_model
        self.d_state = d_state
        self.dropout = dropout
        self.use_parallel_scan = use_parallel_scan
        
        # Initialize A, B, C parameters
        # A is initialized to a negative value to ensure stability
        self.A_log = nn.Parameter(torch.randn(self.d_state))
        self.register_buffer("A_log_scale", torch.ones(1) * math.log(dt_scale))
        
        # B and C are initialized with normal distribution
        # Use Kaiming initialization for better performance on H100
        self.B = nn.Parameter(torch.randn(self.d_model, self.d_state) / math.sqrt(self.d_state))
        self.C = nn.Parameter(torch.randn(self.d_state, self.d_model) / math.sqrt(self.d_state))
        
        # Initialize dt (discretization step size)
        if dt_init == "random":
            # Initialize with random values in [dt_min, dt_max]
            dt = torch.exp(
                torch.rand(self.d_model) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)
            )
            # Ensure dt is at least dt_init_floor
            dt = torch.maximum(dt, torch.tensor(dt_init_floor))
            self.dt_log = nn.Parameter(torch.log(dt))
        else:
            # Initialize with constant value
            dt = torch.ones(self.d_model) * dt_init
            self.dt_log = nn.Parameter(torch.log(dt))
        
        # D is the skip connection parameter
        self.D = nn.Parameter(torch.randn(self.d_model))
        
        # Dropout for regularization
        self.dropout_layer = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()
    
    def forward(self, u, delta=None):
        """
        Forward pass of the Selective SSM.
        
        Args:
            u: Input tensor of shape (batch_size, seq_len, d_model)
            delta: Optional time delta for discretization
            
        Returns:
            y: Output tensor of shape (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, _ = u.shape
        
        # Get discretization step size
        if delta is None:
            # Use the learned dt
            dt = torch.exp(self.dt_log)
        else:
            # Use the provided delta
            dt = delta
        
        # Compute continuous-time parameters
        A = -torch.exp(self.A_log + self.A_log_scale)  # (d_state,)
        
        # Discretize the system using ZOH (zero-order hold)
        # A_bar = exp(A * dt)
        A_bar = torch.exp(rearrange(A, "n -> n 1") * rearrange(dt, "d -> 1 d"))  # (d_state, d_model)
        
        # B_bar = (exp(A * dt) - I) / A * B
        # Compute this carefully to avoid numerical issues
        # When A is close to 0, we use the first-order approximation: B_bar â‰ˆ dt * B
        B_bar = rearrange(self.B, "d n -> d n 1") * rearrange(
            (torch.exp(rearrange(A, "n -> n 1") * rearrange(dt, "d -> 1 d")) - 1.0) / 
            rearrange(A, "n -> n 1"),
            "n d -> 1 n d"
        )  # (d_model, d_state, d_model)
        
        # Prepare for selective scan
        # Compute input-dependent B and C
        u_B = torch.einsum("bld,dne->blne", u, B_bar)  # (batch, seq_len, d_state, d_model)
        u_C = torch.einsum("bld,nd->bln", u, self.C)  # (batch, seq_len, d_state)
        
        if self.use_parallel_scan and seq_len > 1 and torch.cuda.is_available():
            # Use parallel scan for faster computation on GPU
            # Define the scan function
            def scan_fn(h, u_B_t):
                # h: (batch, d_state)
                # u_B_t: (batch, d_state, d_model)
                h_next = torch.einsum("nd,bn->bd", A_bar, h) + u_B_t.sum(dim=-1)
                return h_next
            
            # Initialize hidden state
            h_init = torch.zeros(batch_size, self.d_state, device=u.device)
            
            # Reshape u_B for parallel scan
            u_B_reshaped = u_B.view(batch_size, seq_len, self.d_state * self.d_model)
            
            # Apply parallel scan
            h_all = self._parallel_scan(scan_fn, u_B_reshaped, h_init)
            
            # Compute output
            y = torch.einsum("bln,bln->bl", h_all, u_C)
            
            # Reshape to match expected output
            y = y.unsqueeze(-1).expand(-1, -1, self.d_model)
        else:
            # Fallback to sequential scan for CPU or short sequences
            # Initialize hidden state
            h = torch.zeros(batch_size, self.d_state, device=u.device)  # (batch, d_state)
            
            # Perform the selective scan (sequential)
            ys = []
            for t in range(seq_len):
                # Update hidden state: h_t = A_bar * h_{t-1} + u_B_t
                h = torch.einsum("nd,bn->bd", A_bar, h) + u_B[:, t].sum(dim=-1)  # (batch, d_state)
                
                # Compute output: y_t = C * h_t
                y = u_C[:, t]  # (batch, d_state)
                
                ys.append(y)
            
            # Stack outputs
            y = torch.stack(ys, dim=1)  # (batch, seq_len, d_state)
        
        # Apply skip connection with D
        y = y + u * self.D
        
        # Apply dropout
        y = self.dropout_layer(y)
        
        return y
    
    def _parallel_scan(self, f, x, init):
        """
        Parallel scan implementation using PyTorch's built-in operations.
        
        This is optimized for H100 GPUs by using tensor cores and avoiding
        explicit loops where possible.
        
        Args:
            f: Binary associative function
            x: Input tensor of shape (batch_size, seq_len, ...)
            init: Initial state
            
        Returns:
            Output tensor of shape (batch_size, seq_len, ...)
        """
        # For now, use the reference implementation
        # In a production environment, this would be replaced with a CUDA kernel
        # or a more optimized implementation using PyTorch's built-in operations
        return parallel_scan_ref(f, x, init)


class SelectiveSSMBlock(nn.Module):
    """
    Selective SSM Block with input projection and gating.
    
    This implements a complete Mamba block with:
    1. Input projection
    2. Selective SSM
    3. Output gating
    
    Optimized for H100 GPUs with tensor cores.
    """
    
    def __init__(
        self,
        d_model,
        d_state=64,
        expand_factor=2,
        dropout=0.0,
        use_parallel_scan=True,
    ):
        """
        Initialize the Selective SSM Block.
        
        Args:
            d_model: Model dimension
            d_state: State dimension
            expand_factor: Expansion factor for the intermediate dimension
            dropout: Dropout rate
            use_parallel_scan: Whether to use parallel scan for faster computation
        """
        super().__init__()
        
        self.d_model = d_model
        self.d_state = d_state
        self.expand_factor = expand_factor
        self.d_inner = int(d_model * expand_factor)
        
        # Input projection
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)  # for SSM and gate
        
        # Layer normalization
        self.norm = nn.LayerNorm(d_model)
        
        # Selective SSM
        self.ssm = SelectiveSSM(
            d_model=self.d_inner,
            d_state=d_state,
            dropout=dropout,
            use_parallel_scan=use_parallel_scan,
        )
        
        # Output projection
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)
    
    def forward(self, x):
        """
        Forward pass of the Selective SSM Block.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, d_model)
            
        Returns:
            Output tensor of shape (batch_size, seq_len, d_model)
        """
        # Apply layer normalization
        x_norm = self.norm(x)
        
        # Input projection
        x_proj = self.in_proj(x_norm)
        x_ssm, x_gate = torch.chunk(x_proj, 2, dim=-1)
        
        # Apply SSM
        y_ssm = self.ssm(x_ssm)
        
        # Apply SiLU gate
        y_gated = y_ssm * F.silu(x_gate)
        
        # Output projection
        y = self.out_proj(y_gated)
        
        # Residual connection
        return x + y 